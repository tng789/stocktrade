# train_with_validation.py
import numpy as np
import pandas as pd
import d3rlpy

from d3rlpy.algos import CQL, CQLConfig
from d3rlpy.logging import TensorboardAdapterFactory
from d3rlpy.preprocessing import StandardObservationScaler, MinMaxActionScaler  #, MultiplyRewardScaler

# from simpletradingenv import SimpleTradingEnv  # å¤ç”¨ä½ çš„ç¯å¢ƒ
from enhancedtradingenv import EnhancedTradingEnv  # å¤ç”¨ä½ çš„ç¯å¢ƒ
# from concat_npz import load_all_data_for_stock

import tomllib
from pathlib import Path
import sys
from datetime import datetime
import json

import argparse

# from d3rlpy.metrics import TDErrorEvaluator

def create_val_env_from_ohlcv(df_val, window_size=10, fee_rate=0.001, base_slippage=0.0002, initial_balance=1e6):
    
    """ä¸ºéªŒè¯åˆ›å»º Gym ç¯å¢ƒï¼ˆç”¨äº evaluate_on_environmentï¼‰, å·²åºŸå¼ƒï¼Œä¿ç•™åªä¸ºkwargsç”¨æ³•"""
    
#    env_kwargs = {
#        "initial_cash": 100_000,
#        "commission_buy": 0.0003,
#        "commission_sell": 0.0013,
#        "rebalance_band": 0.05,
#        "take_profit_pct": 0.25,
#        "stop_loss_pct": 0.25,
#        "enable_tplus1": False,  # æ—¥é¢‘æ— éœ€ T+1
#        "window_size": 60
#    }   
#    
    # return MiddleTradingEnv( df=df_val, **env_kwargs)
    return EnhancedTradingEnv(df=df_val)

def buy_hold(prices:pd.Series,initial_cash:int=100000, window_size:int = 60)->dict:
    
    trading_days = 250

    if len(prices) < window_size:
        raise ValueError("price_series too short for window_size")

    # æœ€å‰é¢59å¤©ä¸ºå†å²æ•°æ®ï¼Œç¬¬60å¤©æ˜¯ç¬¬ä¸€å¤©æ­£å¼æ•°æ®ï¼Œä¸è®­ç»ƒä¿æŒåŒæ­¥
    # ä»¥ç¬¬ä¸€å¤©çš„æ­£å¼æ•°æ®å…¨ä»“ä¹°å…¥å¹¶æŒæœ‰, ä¸è®¡ä½£é‡‘å’Œæ»‘ç‚¹
    shares = initial_cash / prices[window_size-1]  
    
    bh_values = shares * prices
    bh_returns = np.diff(bh_values) / bh_values[:-1]
    bh_returns = np.nan_to_num(bh_returns)

    bh_annual_return = np.mean(bh_returns) * trading_days            #æŒ‰ç…§1å¹´250äº¤æ˜“æ—¥è®¡ç®—

    bh_sharpe = np.mean(bh_returns) / (np.std(bh_returns) + 1e-8) * np.sqrt(trading_days)

    bh_cummax = np.maximum.accumulate(bh_values)
    bh_drawdown = (bh_cummax - bh_values) / bh_cummax
    bh_max_drawdown = np.max(bh_drawdown)
    
    metrics = {
        "annual_return": bh_annual_return,
        "sharpe": bh_sharpe,
        "max_drawdown": bh_max_drawdown
    }
    return metrics

# ==============================
# 2. è‡ªå®šä¹‰è¯„ä¼°å‡½æ•°ï¼ˆè®¡ç®—é‡‘èæŒ‡æ ‡ + Buy & Hold åŸºå‡†ï¼‰
# ==============================
def financial_evaluator(env, algo, data_len: int = 90):  #episode_lenæ˜¯æ¯ä¸ªéªŒè¯é›†çš„é•¿åº¦
    """
    åœ¨ç»™å®š env ä¸Šè¿è¡Œç­–ç•¥ï¼Œè¿”å›é‡‘èæŒ‡æ ‡
    """
    window_size = 60
    if data_len < window_size:
        raise RuntimeError('validation dataset length must be greater than 60. 60 for window size') 

    # envä¸­çš„dfæ˜¯æ•´ä¸ªéªŒè¯é›†ï¼Œè€Œä¸”æœ‰æŠ€æœ¯æŒ‡æ ‡ ä¸”æ˜¯å½’ä¸€åŒ–ä¹‹åçš„
    start = 0
    total_days = env.df.shape[0]

    obs = env.reset(start_idx=start, data_length=total_days)        #data_lengthå«window_sizeå’Œå®é™…éªŒè¯æ•°æ®
    print(f"{start=} {env.data_length=} {env.t=}")

    total_values = []
    actions = []
    
    for kk in range(window_size,total_days):                        #å®é™…éªŒè¯æ•°æ®çš„é•¿åº¦
        # action = policy(obs.reshape(1, -1))[0]  # d3rlpy policy è¾“å‡º [1,1]
        # action = algo.predict(obs)  # ç¡®å®šæ€§ç­–ç•¥ï¼Œshape (1,)
        
        # ç­–ç•¥ï¼šåœ¨å¸‚åœºè¶‹åŠ¿å¤§äºé˜ˆå€¼ï¼Œä¹Ÿå°±æ˜¯å‘å¥½çš„æƒ…å†µä¸‹ï¼Œåˆ©ç”¨RLç­–ç•¥(ç›®æ ‡æ˜¯æ¯”ä¹°å…¥æŒæœ‰è¦å¥½äº›)æ¥ç²¾ç¡®æ§åˆ¶ï¼Œåœ¨å¸‚åœºè¶‹åŠ¿ç–²è½¯ä¸æ™¯æ°”æ—¶ï¼Œæœ‰ä¸‰ç§ç­–ç•¥ï¼š
        # 1. ä¸äº¤æ˜“ï¼Œå†»ç»“äº¤æ˜“
        # 2. æ¸…ä»“ï¼ŒæŒå¸è§‚æœ›ï¼Œå¾…å¸‚åœºå¥½èµ·æ¥å†åŠ¨æ‰‹ä¹°å…¥è·Ÿè¿›
        # 3. ä¹°å…¥æŒæœ‰ï¼Œé•¿æœŸå‘å¥½çš„æƒ…å†µä¸‹ï¼Œçœ¼å‰ä¸æ˜æœ—ï¼Œåƒå¸‚åœºbeta
        if env.trend_score > env.trend_threshold:
            action = algo.predict(obs[None,:])[0]  # ç¡®å®šæ€§ç­–ç•¥ï¼Œshape (1,)
        else:
            action = [1.0]                # è¿™ä¸ªåœ°æ–¹æˆ‘è¿˜æ˜¯ç–‘é—®,é•¿æœŸå‘ä¸Šèµ„äº§ ï¼ˆå¦‚ï¼šæ²ªæ·±300ã€æ ‡æ™®500ã€BTCï¼‰,ç½®ä¸º1ï¼Œä¸çœ‹å¥½ï¼Œç½®0
        
        actions.append(action)
        obs, reward, done, info = env.step(action[0])
        # print(f"{kk=} {action=} {reward=} {done=} {env.t=}") 
        total_values.append(info["total_value"])
        if done:
            break
    
    total_values = np.array(total_values)
    returns = np.diff(total_values) / total_values[:-1]
    returns = np.nan_to_num(returns)
    
    if len(returns) == 0:
        return {"sharpe": 0, "annual_return": 0, "max_drawdown": 0}
    
    # å¹´åŒ–æ”¶ç›Šï¼ˆå‡è®¾ 250 äº¤æ˜“æ—¥ï¼‰
    annual_return = np.mean(returns) * 250
    # å¤æ™®æ¯”ç‡ï¼ˆæ— é£é™©åˆ©ç‡=0ï¼‰
    sharpe = np.mean(returns) / (np.std(returns) + 1e-8) * np.sqrt(250)
    # æœ€å¤§å›æ’¤
    cummax = np.maximum.accumulate(total_values)
    drawdown = (cummax - total_values) / cummax
    max_drawdown = np.max(drawdown)

    metrics = {
        "annual_return": annual_return,
        "sharpe": sharpe,
        "max_drawdown": max_drawdown,
        "avg_position": float(np.mean(actions)),
        "final_value":total_values[-1]
        
    } 
    
    return metrics

# --- Buy & Hold åŸºå‡† ---
#    bh_initial_value = env.initial_cash
#    # shares = bh_initial_value / prices[0]  # å…¨ä»“ä¹°å…¥å¹¶æŒæœ‰
#    shares = bh_initial_value / env.price_series[env.window_size-1]  # å…¨ä»“ä¹°å…¥å¹¶æŒæœ‰
#    bh_values = shares * env.price_series
#    bh_returns = np.diff(bh_values) / bh_values[:-1]
#    bh_returns = np.nan_to_num(bh_returns)
#
#    bh_annual_return = np.mean(bh_returns) * 250
#    bh_sharpe = np.mean(bh_returns) / (np.std(bh_returns) + 1e-8) * np.sqrt(250)
#    bh_cummax = np.maximum.accumulate(bh_values)
#    bh_drawdown = (bh_cummax - bh_values) / bh_cummax
#    bh_max_drawdown = np.max(bh_drawdown)
#
#    # --- è¶…é¢æ”¶ç›Šï¼ˆAlphaï¼‰---
#    alpha_over_bh = annual_return - bh_annual_return
#
#    results.append( {
#        "sharpe": float(sharpe),
#        "annual_return": float(annual_return),
#        "max_drawdown": float(max_drawdown),
#        "final_value": float(total_values[-1]),
#        "avg_position": float(np.mean(actions)),
#        # Buy & Hold åŸºå‡†
#        "bh_annual_return": float(bh_annual_return),
#        "bh_sharpe": float(bh_sharpe),
#        "bh_max_drawdown": float(bh_max_drawdown),
#        # è¶…é¢è¡¨ç°
#        "alpha_over_bh": float(alpha_over_bh),
#    })
#
#    start += 15    
#
#    return results

def financial_evaluator_(env, algo, data_len: int = 90):  #episode_lenæ˜¯æ¯ä¸ªéªŒè¯é›†çš„é•¿åº¦
    """
    åœ¨ç»™å®š env ä¸Šè¿è¡Œç­–ç•¥ï¼Œè¿”å›é‡‘èæŒ‡æ ‡
    """
    window_size = 60
    if data_len < window_size:
        raise RuntimeError('validation dataset length must be greater than 60. 60 for window size') 

    # envä¸­çš„dfæ˜¯æ•´ä¸ªéªŒè¯é›†ï¼Œè€Œä¸”æœ‰æŠ€æœ¯æŒ‡æ ‡ ä¸”æ˜¯å½’ä¸€åŒ–ä¹‹åçš„
    start = 0
    total = env.df.shape[0]

    results = []

    while start < total-data_len + 1:    
        # length  = total-start if total-start < data_len else data_len
        # print(f"{length=}")
    
        obs = env.reset(start_idx=start, data_length=data_len)        #data_lengthå«window_sizeå’Œå®é™…éªŒè¯æ•°æ®
        print(f"{start=} {env.data_length=} {env.t=}")

        total_values = []
        actions = []
        
        for kk in range(data_len-window_size):                        #å®é™…éªŒè¯æ•°æ®çš„é•¿åº¦
            # action = policy(obs.reshape(1, -1))[0]  # d3rlpy policy è¾“å‡º [1,1]
            # action = algo.predict(obs)  # ç¡®å®šæ€§ç­–ç•¥ï¼Œshape (1,)
            if env.trend_score > env.trend_threshold:

                action = algo.predict(obs[None,:])[0]  # ç¡®å®šæ€§ç­–ç•¥ï¼Œshape (1,)
                # assert action >= 0, "action must be greater than 0"
                # assert action[0] >=  0,  "Action must be greater than zero"
            else:
                action = [1]
            
            # action[0]  = np.clip(action[0], 0.0, 1.0)
            # actions.append(action.item())
            actions.append(action)
            obs, reward, done, info = env.step(action[0])
            # print(f"{kk=} {action=} {reward=} {done=} {env.t=}") 
            total_values.append(info["total_value"])
            if done:
                break
        
        total_values = np.array(total_values)
        returns = np.diff(total_values) / total_values[:-1]
        returns = np.nan_to_num(returns)
        
        if len(returns) == 0:
            return {"sharpe": 0, "annual_return": 0, "max_drawdown": 0}
        
        # å¹´åŒ–æ”¶ç›Šï¼ˆå‡è®¾ 250 äº¤æ˜“æ—¥ï¼‰
        annual_return = np.mean(returns) * 250
        # å¤æ™®æ¯”ç‡ï¼ˆæ— é£é™©åˆ©ç‡=0ï¼‰
        sharpe = np.mean(returns) / (np.std(returns) + 1e-8) * np.sqrt(250)
        # æœ€å¤§å›æ’¤
        cummax = np.maximum.accumulate(total_values)
        drawdown = (cummax - total_values) / cummax
        max_drawdown = np.max(drawdown)
        
    
    # --- Buy & Hold åŸºå‡† ---
        bh_initial_value = env.initial_cash
        # shares = bh_initial_value / prices[0]  # å…¨ä»“ä¹°å…¥å¹¶æŒæœ‰
        shares = bh_initial_value / env.price_series[env.window_size-1]  # å…¨ä»“ä¹°å…¥å¹¶æŒæœ‰
        bh_values = shares * env.price_series
        bh_returns = np.diff(bh_values) / bh_values[:-1]
        bh_returns = np.nan_to_num(bh_returns)
    
        bh_annual_return = np.mean(bh_returns) * 250
        bh_sharpe = np.mean(bh_returns) / (np.std(bh_returns) + 1e-8) * np.sqrt(250)
        bh_cummax = np.maximum.accumulate(bh_values)
        bh_drawdown = (bh_cummax - bh_values) / bh_cummax
        bh_max_drawdown = np.max(bh_drawdown)
    
        # --- è¶…é¢æ”¶ç›Šï¼ˆAlphaï¼‰---
        alpha_over_bh = annual_return - bh_annual_return

        results.append( {
            "sharpe": float(sharpe),
            "annual_return": float(annual_return),
            "max_drawdown": float(max_drawdown),
            "final_value": float(total_values[-1]),
            "avg_position": float(np.mean(actions)),
            # Buy & Hold åŸºå‡†
            "bh_annual_return": float(bh_annual_return),
            "bh_sharpe": float(bh_sharpe),
            "bh_max_drawdown": float(bh_max_drawdown),
            # è¶…é¢è¡¨ç°
            "alpha_over_bh": float(alpha_over_bh),
        })

        start += 15    

    return results

def parse_opt():
    parser = argparse.ArgumentParser()

    parser.add_argument("--code","-c", type=str, required=True, help="the datafile in csv")
    parser.add_argument("--epochs","-e", type=int, default=60, help="continue to train for more epochs by ignoring the number in toml")
    # parser.add_argument("--yes", "-y", action='store_true', help="cotniue to generate offline data")
    opt = parser.parse_args()
    return opt

def main():
    opt = parse_opt()
    share_code = opt.code
    # ==============================
    # 1. é…ç½®è·¯å¾„ï¼ˆè¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ï¼‰
    # ==============================
    with open(f"{share_code}.toml", "rb") as f:
        cfg = tomllib.load(f)
    
    home_dir = Path(".") / cfg['dataset_dir']/share_code 
    home_dir.mkdir(exist_ok=True)

    val_dataset = home_dir /f"{share_code}.val.csv"    # éªŒè¯æœŸåŸå§‹ OHLCVï¼ˆç”¨äºæ„å»ºå®Œæ•´ episode å›æµ‹ï¼‰
    if not Path(val_dataset).exists():
        raise FileExistsError("validation dataset not exists")

#    # ==============================
#    # 2. åŠ è½½æ•°æ®é›†
#    # ==============================
#    print("ğŸ“‚ åŠ è½½è®­ç»ƒé›†å’ŒéªŒè¯é›†...")
#    train_dataset = load_all_data_for_stock(TRAIN_NPZ)
#    train_dataset.dump("trading_dataset_v2.h5")

    # train_dataset = MDPDataset.load("trading_dataset_v3.h5", buffer=buffer)
    # train_dataset = d3rlpy.dataset.ReplayBuffer.load("./dataset/sz.000513/train_dataset.h5", d3rlpy.dataset.InfiniteBuffer()) 
    train_dataset_file = home_dir / f"{share_code}_train_dataset.h5"
    train_dataset = d3rlpy.dataset.ReplayBuffer.load(train_dataset_file, d3rlpy.dataset.InfiniteBuffer()) 
#    print("è®­ç»ƒé›†æ•°æ®çš„åŠ¨ä½œç©ºé—´: ", train_dataset.dataset_info.action_space)
#    
    # print("Dataset size:", train_dataset.size())
    print("Dataset episode size:", len(train_dataset.episodes))
#    # print(f" - Avg episode length: {train_dataset.size()/ len(train_dataset.episodes):.1f}")
#
    all_rewards = np.concatenate([ep.rewards for ep in train_dataset.episodes])
    all_actions = np.concatenate([ep.actions for ep in train_dataset.episodes])
    all_obs     = np.concatenate([ep.observations for ep in train_dataset.episodes])
#
#    # has_negatives = np.any(all_actions < 0)
#
    print("è®­ç»ƒé›† Dataset shape (rewards):", all_rewards.shape)
    print("å¥–åŠ±Reward mean/std:", all_rewards.mean(), all_rewards.std())
    print(all_rewards.min(), all_rewards.max())
    # print("Dataset shape (actions):", all_actions.shape)
    print("åŠ¨ä½œ/Weight Actions mean/std:", all_actions.mean(), all_actions.std())
    print(f" - Action < 0.3 ratio: {(all_actions < 0.3).mean():.4%}")
    print(f" - Action < 0.5 ratio: {(all_actions < 0.5).mean():.4%}")
    print(f" - Action > 0.7 ratio: {(all_actions > 0.7).mean():.2%}")
    print(f" - Action > 0.8 ratio: {(all_actions > 0.8).mean():.2%}")
    print(f" - Action > 0.9 ratio: {(all_actions > 0.9).mean():.2%}")
    print("ç¯å¢ƒç©ºé—´Observation mean/std:", all_obs.mean(), all_obs.std())
#    
    # ==============================
    # 3. åˆ›å»ºéªŒè¯ç¯å¢ƒï¼ˆç”¨äºå®Œæ•´ episode è¯„ä¼°ï¼‰
    # ==============================

    # with h5py.File('trading_dataset_v3.h5', 'r') as f:
        # train_dataset = f['dataset']
#        observations = np.array(f['observations'])
#        actions = np.array(f['actions'])
#        rewards = np.array(f['rewards'])
#        terminals = np.array(f['terminals'])  # æˆ– 'dones'ï¼Œå¸ƒå°”ç±»å‹
#
    # 2. åˆ›å»ºMDPDataset
#    dataset = MDPDataset(
#        observations=observations,
#        actions=actions,
#        rewards=rewards,
#        terminals=terminals,
#        # discrete_action=False  # æ ¹æ®ä½ çš„åŠ¨ä½œç©ºé—´è®¾ç½®ï¼šFalseä¸ºè¿ç»­ï¼ŒTrueä¸ºç¦»æ•£
#    )

    print("ğŸ› ï¸  åˆ›å»ºéªŒè¯ç¯å¢ƒ...")
    df_val = pd.read_csv(val_dataset)
    metrics_bh = buy_hold(df_val['CLOSE'])

    print("ğŸ› ï¸ éªŒè¯é›†æ•°æ®æŒ‡æ ‡ï¼š")
    print(json.dumps(metrics_bh,indent=4))

    # val_env = create_val_env_from_ohlcv( df_val, window_size=60, fee_rate=0.001, base_slippage=0.0005)
    
    val_env = EnhancedTradingEnv(df=df_val, mode="predict")

    # val_env.reset(0, df_val.shape[0])

    # 1. å®šä¹‰éªŒè¯å›è°ƒï¼ˆæ¯è½®è¯„ä¼°ä¸€æ¬¡ï¼‰
    # evaluator = EnvironmentEvaluator(val_env, n_trials=10)    ## å•æ¬¡ episode éªŒè¯ï¼ˆå› æ˜¯ determinstic tradingï¼‰ 
    #evaluators = {
    #    "environment_reward": financial_evaluator,  # è¿™ä¼šè®¡ç®—å¹³å‡ Reward
        # ä½ ä¾ç„¶å¯ä»¥ä¿ç•™ä¹‹å‰çš„ç¦»çº¿æŒ‡æ ‡ä½œä¸ºå‚è€ƒ
    #} 
        # env=val_env,   # ä½ çš„éªŒè¯ç¯å¢ƒï¼ˆ2024å¹´æ•°æ®ï¼‰
    
    d3rlpy.seed(53)
    # ==============================
    # 4. åˆå§‹åŒ– CQL æ¨¡å‹
    # ==============================
    # scaler = StandardObservationScaler()
    #td3_config = TD3Config(
    #    actor_learning_rate=3e-4,
    #    critic_learning_rate=3e-4,
    #    batch_size=128,
    #    observation_scaler = StandardObservationScaler(),
    #    action_scaler=MinMaxActionScaler(minimum=0.0, maximum=1.0),
    #    reward_scaler=MultiplyRewardScaler(100)
    #)
    #.create(device='cpu',enable_ddp=False)
    cql_config = CQLConfig(
        actor_learning_rate=3e-4,
        critic_learning_rate=3e-4,
        conservative_weight=1.0,        #è¶Šä½è¶Šä¸ä¿å®ˆï¼Œè¶Šèƒ½æ¢ç´¢ 
        batch_size=256,
        alpha_threshold = 10.0,
        initial_alpha = 0.2,
        observation_scaler=StandardObservationScaler(),
        # action_scaler=MinMaxActionScaler(minimum=0.0, maximum=1.0),
        action_scaler=MinMaxActionScaler(),
        # reward_scaler=MultiplyRewardScaler(100)
    )

    cql = CQL(cql_config,device='cuda:0',enable_ddp=False)
    # cql = CQL(cql_config,device='cpu',enable_ddp=False)
    
    # cql.build_with_dataset(train_dataset) 

    # ==============================
    # 5. å®šä¹‰éªŒè¯æŒ‡æ ‡ï¼šåœ¨å®Œæ•´éªŒè¯ episode ä¸Šçš„æ€»å›æŠ¥
    # ==============================
#    def validation_return_scorer(algo, dataset):
#        """åŒ…è£… evaluate_on_environment ä»¥é€‚é… scorer æ¥å£"""
#        # eval_score = evaluate_qlearning_with_environment(val_env,n_trials=10)
#        # return eval_score(algo)
#        return  evaluate_qlearning_with_environment(algo, val_env,n_trials=10)
#
#    scorers = {
#        'validation_return': validation_return_scorer,
#        # å¯é€‰ï¼šæ·»åŠ å…¶ä»–æŒ‡æ ‡
#        # 'td_error': td_error_scorer,
#    }
#    
    def validation_scorer(algo, val_env):

        metrics = financial_evaluator(val_env, algo, data_len=val_env.df.shape[0] )           #, episode_len=len(val_env.price_series))
        # print(f"\n[Step {step}] Val Metrics: {metrics}")
        print("Final Validation Metrics:")
        result_str = json.dumps(metrics, indent=4)
        print(result_str)
        
        # return metrics["final_value"]  # ä»¥å‡€å€¼ä¸ºä¼˜åŒ–ç›®æ ‡
        return metrics

#    def log_loss(algo, epoch, total_step, *args, **kwargs):
#        if total_step % 500 == 0:
#            critic_loss = algo.learn_info.get("critic_loss", float("nan"))
#            actor_loss = algo.learn_info.get("actor_loss", float("nan"))
#            print(f"[Step {total_step:6d}] Critic: {critic_loss:.6f} | Actor: {actor_loss:.6f}") 
            
    # ==============================
    # 5. è®­ç»ƒå›è°ƒï¼šæ¯è½®éªŒè¯ + ä¿å­˜æœ€ä½³
    # ==============================
    best_score = -np.inf
    # patience = 10
    latest_model_num =0
    patience_counter = 0
    best_epoch = 0
    # best_model_path = os.path.join(MODEL_SAVE_DIR, "cql_best_val_return.d3")

    def epoch_callback(algo, epoch, total_step):
        nonlocal best_score, patience_counter, metrics_bh, best_epoch, latest_model_num
        # current_time = datetime.now().strftime("%Y%m%d%H%M%S")
        # model_path = home_dir/f"{share_code}.{current_time}.{epoch:03d}.d3"
        print(f"{latest_model_num=}")
        model_path = home_dir/f"{share_code}.{epoch+latest_model_num:03d}.d3"

        algo.save(model_path)
        print(f"ğŸ‰ æ–°æ¨¡å‹ä¿å­˜è‡³: {model_path}")

        return 

#        print(f"\n[Epoch {epoch}] å¼€å§‹éªŒè¯...")
#        
#        val_score = validation_scorer(algo, val_env)
#        
#        score = val_score['annual_return'] - metrics_bh['annual_return']
#        print(f"score of the epoch: {score}")
#        if score > best_score:
#            best_score = score
#            val_score['epoch'] = epoch
#            best_epoch = epoch
#        
#            with open(home_dir/f"{current_time}.json","wt") as f:
#                json.dump(val_score,f)
#
#            patience_counter = 0
#        else:
#            patience_counter += 1
#            if patience_counter >= patience:
#                print("â¹ï¸ éªŒè¯å›æŠ¥è¿ç»­ä¸‹é™ï¼Œå»ºè®®æ—©åœï¼ˆå½“å‰ç‰ˆæœ¬æ— æ³•ä¸­æ–­ï¼Œç»§ç»­è®­ç»ƒ...ï¼‰")
#        print(f"[Epoch {epoch}]ç»“æŸã€‚ å½“å‰æœ€ä½³éªŒè¯å‘ç”Ÿåœ¨ç¬¬ {best_epoch} è½®ï¼Œå¾—åˆ† Alpha {best_score:.4f}\n")

    # ==============================
    # 6. è®­ç»ƒ + éªŒè¯ + ä¿å­˜æœ€ä½³æ¨¡å‹
    # ==============================
    print("ğŸš€ å¼€å§‹è®­ç»ƒ cql...")

    # 4. è®¾ç½®è®­ç»ƒå‚æ•°
    # ==============================

    # print(f"ğŸ“Š batch_size={batch_size}, dataset_size={dataset_size}")
    # print(f"ğŸ”„ n_steps_per_epoch = {n_steps_per_epoch} (â‰ˆ1 full pass per epoch)")        

    # factory = TensorboardAdapterFactory(root_dir="d3rlpy_logs", experiment_name= f"{share_code}.{datetime.now().strftime("%Y%m%d")}")
    
    n_steps_per_epoch = 5000     #train_dataset.size() è¿”å›çš„æ˜¯episodeçš„æ•°é‡ï¼Œè‚¯å®šä¸å¯¹ï¼Œç”¨transactionçš„æ•°é‡ä¹Ÿä¸å¥½ï¼Œä»å»ºè®®å›ºå®šä¸€ä¸ªæ•°å­—ï¼Œæš‚å–10000

    if opt.epochs is None:                          # æ— epochsï¼Œå°±æ˜¯æŒ‰ç…§tomlæ‰§è¡Œ
        print("epochs from cmd line not provided")
        # for epoch in range(cql_config..n_epochs):
        n_epochs = cfg['train_epochs']                  # è½®æ•°,æœ€ä½³æ¨¡å‹å¾€å¾€å‡ºç°åœ¨å‰10~30è½®ã€‚
        total_steps = n_epochs * n_steps_per_epoch

        cql = CQL(cql_config,device='cuda:0',enable_ddp=False)

    else:   # æœ‰epochsåˆ™æŒ‰ç…§å‘½ä»¤è¡Œå‚æ•°æ¥ï¼ŒæŒ‡åœ¨åŸå…ˆæ¨¡å‹ä¸Šç»§ç»­è®­ç»ƒ
        print(f"epochs from cmd line {opt.epochs}")
        n_epochs = opt.epochs                  # è½®æ•°,æœ€ä½³æ¨¡å‹å¾€å¾€å‡ºç°åœ¨å‰10~30è½®ã€‚
        total_steps = n_epochs * n_steps_per_epoch

        # æ‰¾åˆ°æœ€æ–°çš„é‚£ä¸ªæ¨¡å‹...
        models = sorted(list(home_dir.glob("*.d3")))
        if len(models) >= 1:
            latest_model = str(models[-1])
            latest_model_num    = int(latest_model.split(".")[-2])
            print(f"{latest_model=} {latest_model_num=}")
            # cql.load_model(latest_model)
            cql = d3rlpy.load_learnable(latest_model,device='cuda:0')
        else:
            # æ²¡æ‰¾åˆ°
            # latest_model_num = 0

            cql = CQL(cql_config,device='cuda:0',enable_ddp=False)
            # cql = CQL(cql_config,device='cpu',enable_ddp=False)
    
    cql.build_with_dataset(train_dataset) 
    # ==============================
    learn_info = cql.fit(                  
        train_dataset,
        n_steps= total_steps,
        n_steps_per_epoch=n_steps_per_epoch,
        show_progress= True,
        logger_adapter=TensorboardAdapterFactory(root_dir=f"d3rlpy_logs/{share_code}"),
        # logger_adapter=factory,
        epoch_callback=epoch_callback
        # callback = log_loss
    )
        
    print(learn_info)

    # print(f"âœ… è®­ç»ƒå®Œæˆã€‚æœ€ä½³éªŒè¯å›æŠ¥: {best_score:.4f}")
    print("âœ… è®­ç»ƒå®Œæˆ ")
    # print(f"ğŸ“ æœ€ä½³æ¨¡å‹: {best_model_path}")

if __name__ == "__main__":
    main()
    


# evaluators={"td_error": TDErrorEvaluator(train_dataset)}
#class LossLogger:
#    def __call__(self, algo, epoch, total_step):
#        print(f"Step {total_step}: critic_loss={algo.actor_loss}")
#        
#def load_dataset_from_npz(path):
#    """ä» .npz åŠ è½½ MDPDataset"""
#    data = np.load(path)
#    return  MDPDataset(
#        observations=data['observations'],
#        actions=data['actions'],
#        rewards=data['rewards'],
#        terminals=data['terminals'],
#        action_space=spaces.Box(low=0.0, high=1.0, shape=(1,), dtype=np.float32)
#    )
#